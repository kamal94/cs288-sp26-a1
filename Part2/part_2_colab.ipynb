{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a93e49e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2631500739.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataPoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Multi-layer perceptron model for Assignment 1: Starter code.\n",
    "\n",
    "You can change this code while keeping the function giving headers. You can add any functions that will help you. The given function headers are used for testing the code, so changing them will fail testing.\n",
    "\n",
    "\n",
    "We adapt shape suffixes style when working with tensors.\n",
    "See https://medium.com/@NoamShazeer/shape-suffixes-good-coding-style-f836e72e24fd.\n",
    "\n",
    "Dimension key:\n",
    "\n",
    "b: batch size\n",
    "l: max sequence length\n",
    "c: number of classes\n",
    "v: vocabulary size\n",
    "\n",
    "For example,\n",
    "\n",
    "feature_b_l means a tensor of shape (b, l) == (batch_size, max_sequence_length).\n",
    "length_1 means a tensor of shape (1) == (1,).\n",
    "loss means a tensor of shape (). You can retrieve the loss value with loss.item().\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "from utils import DataPoint, DataType, accuracy, load_data, save_results\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "    # The index of the padding embedding.\n",
    "    # This is used to pad variable length sequences.\n",
    "    TOK_PADDING_INDEX = 0\n",
    "    STOP_WORDS = set(pd.read_csv(\"stopwords.txt\", header=None)[0])\n",
    "\n",
    "    def _pre_process_text(self, text: str) -> List[str]:\n",
    "        # TODO: Implement this! Expected # of lines: 5~10\n",
    "        return [\n",
    "            t.lower() for t in text.split(\" \") if t.lower() not in Tokenizer.STOP_WORDS\n",
    "        ]\n",
    "\n",
    "    def __init__(self, data: List[DataPoint], max_vocab_size: int = None):\n",
    "        corpus = \" \".join([d.text for d in data])\n",
    "        token_freq = Counter(self._pre_process_text(corpus))\n",
    "        token_freq = token_freq.most_common(max_vocab_size)\n",
    "        tokens = [t for t, _ in token_freq]\n",
    "        # offset because padding index is 0\n",
    "        self.token2id = {t: (i + 1) for i, t in enumerate(tokens)}\n",
    "        self.token2id[\"<PAD>\"] = Tokenizer.TOK_PADDING_INDEX\n",
    "        self.id2token = {i: t for t, i in self.token2id.items()}\n",
    "\n",
    "    def tokenize(self, text: str) -> List[int]:\n",
    "        # TODO: Implement this! Expected # of lines: 5~10\n",
    "        return [self.token2id[token] for token in self._pre_process_text(text)]\n",
    "\n",
    "\n",
    "def get_label_mappings(\n",
    "    data: List[DataPoint],\n",
    ") -> Tuple[Dict[str, int], Dict[int, str]]:\n",
    "    \"\"\"Reads the labels file and returns the mapping.\"\"\"\n",
    "    labels = list(set([d.label for d in data]))\n",
    "    label2id = {label: index for index, label in enumerate(labels)}\n",
    "    id2label = {index: label for index, label in enumerate(labels)}\n",
    "    return label2id, id2label\n",
    "\n",
    "\n",
    "class BOWDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: List[DataPoint],\n",
    "        tokenizer: Tokenizer,\n",
    "        label2id: Dict[str, int],\n",
    "        max_length: int = 100,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label2id = label2id\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Returns a single example as a tuple of torch.Tensors.\n",
    "        features_l: The tokenized text of example, shaped (max_length,)\n",
    "        length: The length of the text, shaped ()\n",
    "        label: The label of the example, shaped ()\n",
    "\n",
    "        All of have type torch.int64.\n",
    "        \"\"\"\n",
    "        dp: DataPoint = self.data[idx]\n",
    "        tokenized_input = self.tokenizer.tokenize(dp.text)\n",
    "        if len(tokenized_input) > self.max_length:\n",
    "            tokenized_input = tokenized_input[:self.max_length]\n",
    "        elif len(tokenized_input) < self.max_length:\n",
    "            tokenized_input += [Tokenizer.TOK_PADDING_INDEX] * (self.max_length - len(tokenized_input))\n",
    "        return (\n",
    "            torch.tensor(tokenized_input),\n",
    "            torch.tensor(len(dp.text)),\n",
    "            torch.tensor(int(dp.label)),\n",
    "        )\n",
    "        # TODO: Implement this! Expected # of lines: ~20\n",
    "\n",
    "\n",
    "class MultilayerPerceptronModel(nn.Module):\n",
    "    \"\"\"Multi-layer perceptron model for classification.\"\"\"\n",
    "\n",
    "    EMBEDDING_DIM = 128\n",
    "\n",
    "    def __init__(self, vocab_size: int, num_classes: int, padding_index: int):\n",
    "        \"\"\"Initializes the model.\n",
    "\n",
    "        Inputs:\n",
    "            num_classes (int): The number of classes.\n",
    "            vocab_size (int): The size of the vocabulary.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.padding_index = padding_index\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_classes = num_classes\n",
    "        self.embedding = torch.nn.Embedding(\n",
    "            self.vocab_size, self.EMBEDDING_DIM, padding_idx=self.padding_index\n",
    "        )\n",
    "        self.fc1 = nn.Linear(self.EMBEDDING_DIM, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 1024)\n",
    "        self.fc3 = nn.Linear(1024, self.num_classes)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        # TODO: Implement this!\n",
    "\n",
    "    def forward(\n",
    "        self, input_features_b_l: torch.Tensor, input_length_b: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Forward pass of the model.\n",
    "\n",
    "        Inputs:\n",
    "            input_features_b_l (tensor): Input data for an example or a batch of examples.\n",
    "            input_length (tensor): The length of the input data.\n",
    "\n",
    "        Returns:\n",
    "            output_b_c: The output of the model.\n",
    "        \"\"\"\n",
    "        x_embedding = self.embedding(input_features_b_l)\n",
    "\n",
    "        x = torch.sum(x_embedding, dim=1)\n",
    "        x = x / input_length_b.unsqueeze(1).float()\n",
    "\n",
    "        # print(f\"x_embedding:\", x_embedding.shape)\n",
    "\n",
    "        # shape_x1 = x_embedding.shape[-1]\n",
    "        # shape_x2 = x_embedding.shape[-2]\n",
    "        # print(f\"{shape_x1=}, {shape_x2=}\")\n",
    "        # x = x_embedding.reshape(shape_x1 * shape_x2)\n",
    "\n",
    "        # print(f\"x before fc1:\", x.shape)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # print(f\"x before fc2:\", x.shape)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # print(f\"x before fc3:\", x.shape)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        # print(f\"x before output layer:\", x.shape)\n",
    "        x = torch.log_softmax(x, dim=-1)\n",
    "        # print(\"sum of x:\", torch.sum(x))\n",
    "        return x\n",
    "\n",
    "        # TODO: Implement this!\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model: nn.Module):\n",
    "        self.model = model\n",
    "\n",
    "    def predict(self, data: BOWDataset) -> List[int]:\n",
    "        \"\"\"Predicts a label for an input.\n",
    "\n",
    "        Inputs:\n",
    "            model_input (tensor): Input data for an example or a batch of examples.\n",
    "\n",
    "        Returns:\n",
    "            The predicted class.\n",
    "\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        all_predictions = []\n",
    "        dataloader = DataLoader(data, batch_size=32, shuffle=False)\n",
    "        for features_b_l, lengths_b, labels_b in dataloader:\n",
    "            output_b_c = self.model(features_b_l, lengths_b)\n",
    "            all_predictions.extend(output_b_c.argmax(dim=-1).tolist())\n",
    "        return all_predictions\n",
    "        # TODO: Implement this!\n",
    "\n",
    "    def evaluate(self, data: BOWDataset) -> float:\n",
    "        \"\"\"Evaluates the model on a dataset.\n",
    "\n",
    "        Inputs:\n",
    "            data: The dataset to evaluate on.\n",
    "\n",
    "        Returns:\n",
    "            The accuracy of the model.\n",
    "        \"\"\"\n",
    "        # TODO: Implement this!\n",
    "        self.model.eval()\n",
    "        all_predictions = []\n",
    "        dataloader = DataLoader(data, batch_size=32, shuffle=False)\n",
    "        for features_b_l, lengths_b, labels_b in dataloader:\n",
    "            output_b_c = self.model(features_b_l, lengths_b)\n",
    "            all_predictions.extend(output_b_c.argmax(dim=-1).tolist())\n",
    "        return accuracy(all_predictions, data.labels)\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        training_data: BOWDataset,\n",
    "        val_data: BOWDataset,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        num_epochs: int,\n",
    "    ) -> None:\n",
    "        \"\"\"Trains the MLP.\n",
    "\n",
    "        Inputs:\n",
    "            training_data: Suggested type for an individual training example is\n",
    "                an (input, label) pair or (input, id, label) tuple.\n",
    "                You can also use a dataloader.\n",
    "            val_data: Validation data.\n",
    "            optimizer: The optimization method.\n",
    "            num_epochs: The number of training epochs.\n",
    "        \"\"\"\n",
    "        torch.manual_seed(0)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adagrad(self.model.parameters())\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            self.model.train()\n",
    "            total_loss = 0\n",
    "            dataloader = DataLoader(training_data, batch_size=4, shuffle=True)\n",
    "            for inputs_b_l, lengths_b, labels_b in tqdm(dataloader):\n",
    "              inputs_in_cuda = inputs_b_l.cuda()\n",
    "              lengths_in_cuda = lengths_b.cuda()\n",
    "              labels_in_cuda = torch.tensor(labels_b).cuda()\n",
    "              \n",
    "              # zero the parameter gradients\n",
    "              optimizer.zero_grad()\n",
    "      \n",
    "              # forward + backward + optimize\n",
    "              outputs = self.model(inputs_in_cuda, lengths_in_cuda)\n",
    "              loss = criterion(outputs, labels_in_cuda)\n",
    "              per_dp_loss = loss.item()\n",
    "              total_loss += per_dp_loss\n",
    "              loss.backward()\n",
    "              optimizer.step()\n",
    "\n",
    "            # TODO: Implement this!\n",
    "            val_acc = self.evaluate(val_data)\n",
    "\n",
    "            print(\n",
    "                f\"Epoch: {epoch + 1:<2} | Loss: {per_dp_loss:.2f} | Val accuracy: {100 * val_acc:.2f}%\"\n",
    "            )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"MultiLayerPerceptron model\")\n",
    "    parser.add_argument(\n",
    "        \"-d\",\n",
    "        \"--data\",\n",
    "        type=str,\n",
    "        default=\"sst2\",\n",
    "        help=\"Data source, one of ('sst2', 'newsgroups')\",\n",
    "    )\n",
    "    parser.add_argument(\"-e\", \"--epochs\", type=int, default=3, help=\"Number of epochs\")\n",
    "    parser.add_argument(\n",
    "        \"-l\", \"--learning_rate\", type=float, default=0.001, help=\"Learning rate\"\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    num_epochs = args.epochs\n",
    "    lr = args.learning_rate\n",
    "    data_type = DataType(args.data)\n",
    "\n",
    "    train_data, val_data, dev_data, test_data = load_data(data_type)\n",
    "\n",
    "    tokenizer = Tokenizer(train_data, max_vocab_size=20000)\n",
    "    label2id, id2label = get_label_mappings(train_data)\n",
    "    print(\"Id to label mapping:\")\n",
    "    pprint(id2label)\n",
    "\n",
    "    max_length = 100\n",
    "    train_ds = BOWDataset(train_data, tokenizer, label2id, max_length)\n",
    "    val_ds = BOWDataset(val_data, tokenizer, label2id, max_length)\n",
    "    dev_ds = BOWDataset(dev_data, tokenizer, label2id, max_length)\n",
    "    test_ds = BOWDataset(test_data, tokenizer, label2id, max_length)\n",
    "\n",
    "    model = MultilayerPerceptronModel(\n",
    "        vocab_size=len(tokenizer.token2id),\n",
    "        num_classes=len(label2id),\n",
    "        padding_index=Tokenizer.TOK_PADDING_INDEX,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(model)\n",
    "\n",
    "    print(\"Training the model...\")\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    trainer.train(train_ds, val_ds, optimizer, num_epochs)\n",
    "\n",
    "    # Evaluate on dev\n",
    "    dev_acc = trainer.evaluate(dev_ds)\n",
    "    print(f\"Development accuracy: {100 * dev_acc:.2f}%\")\n",
    "\n",
    "    # Predict on test\n",
    "    test_preds = trainer.predict(test_ds)\n",
    "    test_preds = [id2label[pred] for pred in test_preds]\n",
    "    save_results(\n",
    "        test_data,\n",
    "        test_preds,\n",
    "        os.path.join(\"results\", f\"mlp_{args.data}_test_predictions.csv\"),\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
